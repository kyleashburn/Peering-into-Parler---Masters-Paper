{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f69057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing our libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# libraries that will let us do LDA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "# library that will let us go ahead and visualize LDA results\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "\n",
    "# helpful library to allow for computing coherence using Gensim\n",
    "import tmtoolkit\n",
    "\n",
    "# what we're using to pickle with\n",
    "import pickle\n",
    "\n",
    "# importing json to serialize the thingy\n",
    "import json\n",
    "\n",
    "# import timing library\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14f52fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "febba12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to override the pre-proc that occurs within the vectorizer\n",
    "# just returns the original string -> because I already had it clean\n",
    "def dummy_func(x):   \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebfaf9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cust_tokenizer(x):\n",
    "    return x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b8ec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 13s\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# reading in the data\n",
    "df = pd.read_json(\"min_act_lang_filtered_pre_proc.ndjson\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4329af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a TFIDF vectorizer with unigram representation\n",
    "# it's cheaper and it will allow for relative pruning (those terms appearing within fewer than .05% of docs or in more than 99%)\n",
    "# feeding in our own functions for splitting and cleaning because otherwise it will mess up our plan\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df =.005, max_df = .99,  preprocessor=dummy_func, tokenizer=cust_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c069a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 19s\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# fitting the vectorizer\n",
    "tf = tfidf_vectorizer.fit_transform(df[\"bo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4bd0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# values [7, 10, 15, 20, 25, 30, 40, 60, 80, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62a73f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a tokenized representation of the cleaned column -> this is needed for the coherence calculation\n",
    "df[\"tokens\"] = df[\"bo\"].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61440392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing at various levels of K [7, 10, 15, 20, 25, 30, 40, 60, 80, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09bd2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st time just trying from 10-50 to see how that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eae22195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashbu\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:22: DeprecationWarning: Please use `triu` from the `scipy.linalg` namespace, the `scipy.linalg.special_matrices` namespace is deprecated.\n",
      "  from scipy.linalg.special_matrices import triu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17min 11s\n",
      "Wall time: 1h 26min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initializing the LDA model with a k of 7\n",
    "lda_k7 = LatentDirichletAllocation(\n",
    "    n_components=7,\n",
    "    n_jobs=5,\n",
    "    max_iter=5,\n",
    "    learning_method=\"batch\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# fitting the LDA model\n",
    "lda_k7.fit(tf)\n",
    "\n",
    "# computing the coherence of the topic model with K=7\n",
    "k7_coh = tmtoolkit.topicmod.evaluate.metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=25, \n",
    "                        topic_word_distrib=lda_k7.components_, # the components of the lda count as \n",
    "                        dtm=tf, # the term frequency\n",
    "                        vocab=np.array([x for x in tfidf_vectorizer.vocabulary_.keys()]), # pass in vectorizer\n",
    "                        texts=df[\"tokens\"].values) # pass in list of tokenized texts -> needs to match vocab\n",
    "\n",
    "# saving the model\n",
    "pickle.dump(lda_k7, open(\"min_act_lda_k7.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d750f0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17min 59s\n",
      "Wall time: 1h 22min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initializing the LDA model with a k of 10\n",
    "lda_k10 = LatentDirichletAllocation(\n",
    "    n_components=10,\n",
    "    n_jobs=5,\n",
    "    max_iter=5,\n",
    "    learning_method=\"batch\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# fitting the LDA model\n",
    "lda_k10.fit(tf)\n",
    "\n",
    "# computing the coherence of the topic model with K=10\n",
    "k10_coh = tmtoolkit.topicmod.evaluate.metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=25, \n",
    "                        topic_word_distrib=lda_k10.components_, # the components of the lda count as \n",
    "                        dtm=tf, # the term frequency\n",
    "                        vocab=np.array([x for x in tfidf_vectorizer.vocabulary_.keys()]), # pass in vectorizer\n",
    "                        texts=df[\"tokens\"].values) # pass in list of tokenized texts -> needs to match vocab\n",
    "\n",
    "# saving the model\n",
    "pickle.dump(lda_k10, open(\"min_act_lda_k10.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d189ca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 18min 53s\n",
      "Wall time: 1h 20min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initializing the LDA model with a k of 15\n",
    "lda_k15 = LatentDirichletAllocation(\n",
    "    n_components=15,\n",
    "    n_jobs=5,\n",
    "    max_iter=5,\n",
    "    learning_method=\"batch\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# fitting the LDA model\n",
    "lda_k15.fit(tf)\n",
    "\n",
    "# computing the coherence of the topic model with K=10\n",
    "k15_coh = tmtoolkit.topicmod.evaluate.metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=25, \n",
    "                        topic_word_distrib=lda_k15.components_, # the components of the lda count as \n",
    "                        dtm=tf, # the term frequency\n",
    "                        vocab=np.array([x for x in tfidf_vectorizer.vocabulary_.keys()]), # pass in vectorizer\n",
    "                        texts=df[\"tokens\"].values) # pass in list of tokenized texts -> needs to match vocab\n",
    "\n",
    "# saving the model\n",
    "pickle.dump(lda_k15, open(\"min_act_lda_k15.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34cb7f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 19min 29s\n",
      "Wall time: 1h 19min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initializing the LDA model with a k of 20\n",
    "lda_k20 = LatentDirichletAllocation(\n",
    "    n_components=20,\n",
    "    n_jobs=5,\n",
    "    max_iter=5,\n",
    "    learning_method=\"batch\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# fitting the LDA model\n",
    "lda_k20.fit(tf)\n",
    "\n",
    "# computing the coherence of the topic model with K=20\n",
    "k20_coh = tmtoolkit.topicmod.evaluate.metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=25, \n",
    "                        topic_word_distrib=lda_k20.components_, # the components of the lda count as \n",
    "                        dtm=tf, # the term frequency\n",
    "                        vocab=np.array([x for x in tfidf_vectorizer.vocabulary_.keys()]), # pass in vectorizer\n",
    "                        texts=df[\"tokens\"].values) # pass in list of tokenized texts -> needs to match vocab\n",
    "\n",
    "# saving the model\n",
    "pickle.dump(lda_k20, open(\"min_act_lda_k20.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ce7de98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 20min 5s\n",
      "Wall time: 1h 19min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initializing the LDA model with a k of 25\n",
    "lda_k25 = LatentDirichletAllocation(\n",
    "    n_components=25,\n",
    "    n_jobs=5,\n",
    "    max_iter=5,\n",
    "    learning_method=\"batch\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# fitting the LDA model\n",
    "lda_k25.fit(tf)\n",
    "\n",
    "# computing the coherence of the topic model with K=25\n",
    "k25_coh = tmtoolkit.topicmod.evaluate.metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=25, \n",
    "                        topic_word_distrib=lda_k25.components_, # the components of the lda count as \n",
    "                        dtm=tf, # the term frequency\n",
    "                        vocab=np.array([x for x in tfidf_vectorizer.vocabulary_.keys()]), # pass in vectorizer\n",
    "                        texts=df[\"tokens\"].values) # pass in list of tokenized texts -> needs to match vocab\n",
    "\n",
    "# saving the model\n",
    "pickle.dump(lda_k25, open(\"min_act_lda_k25.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce71076d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 21min 3s\n",
      "Wall time: 1h 19min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initializing the LDA model with a k of 30\n",
    "lda_k30 = LatentDirichletAllocation(\n",
    "    n_components=30,\n",
    "    n_jobs=5,\n",
    "    max_iter=5,\n",
    "    learning_method=\"batch\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# fitting the LDA model\n",
    "lda_k30.fit(tf)\n",
    "\n",
    "# computing the coherence of the topic model with K=30\n",
    "k30_coh = tmtoolkit.topicmod.evaluate.metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=25, \n",
    "                        topic_word_distrib=lda_k30.components_, # the components of the lda count as \n",
    "                        dtm=tf, # the term frequency\n",
    "                        vocab=np.array([x for x in tfidf_vectorizer.vocabulary_.keys()]), # pass in vectorizer\n",
    "                        texts=df[\"tokens\"].values) # pass in list of tokenized texts -> needs to match vocab\n",
    "\n",
    "# saving the model\n",
    "pickle.dump(lda_k30, open(\"min_act_lda_k30.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44eba83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 22min\n",
      "Wall time: 1h 18min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initializing the LDA model with a k of 40\n",
    "lda_k40 = LatentDirichletAllocation(\n",
    "    n_components=40,\n",
    "    n_jobs=5,\n",
    "    max_iter=5,\n",
    "    learning_method=\"batch\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# fitting the LDA model\n",
    "lda_k40.fit(tf)\n",
    "\n",
    "# computing the coherence of the topic model with K=40\n",
    "k40_coh = tmtoolkit.topicmod.evaluate.metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=25, \n",
    "                        topic_word_distrib=lda_k40.components_, # the components of the lda count as \n",
    "                        dtm=tf, # the term frequency\n",
    "                        vocab=np.array([x for x in tfidf_vectorizer.vocabulary_.keys()]), # pass in vectorizer\n",
    "                        texts=df[\"tokens\"].values) # pass in list of tokenized texts -> needs to match vocab\n",
    "\n",
    "# saving the model\n",
    "pickle.dump(lda_k40, open(\"min_act_lda_k40.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dec066c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 23min 44s\n",
      "Wall time: 1h 27min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initializing the LDA model with a k of 60\n",
    "lda_k60 = LatentDirichletAllocation(\n",
    "    n_components=60,\n",
    "    n_jobs=5,\n",
    "    max_iter=5,\n",
    "    learning_method=\"batch\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# fitting the LDA model\n",
    "lda_k60.fit(tf)\n",
    "\n",
    "# computing the coherence of the topic model with K=40\n",
    "k60_coh = tmtoolkit.topicmod.evaluate.metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=25, \n",
    "                        topic_word_distrib=lda_k60.components_, # the components of the lda count as \n",
    "                        dtm=tf, # the term frequency\n",
    "                        vocab=np.array([x for x in tfidf_vectorizer.vocabulary_.keys()]), # pass in vectorizer\n",
    "                        texts=df[\"tokens\"].values) # pass in list of tokenized texts -> needs to match vocab\n",
    "\n",
    "# saving the model\n",
    "pickle.dump(lda_k60, open(\"min_act_lda_k60.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d340735b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 25min 6s\n",
      "Wall time: 1h 29min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# initializing the LDA model with a k of 80\n",
    "lda_k80 = LatentDirichletAllocation(\n",
    "    n_components=80,\n",
    "    n_jobs=5,\n",
    "    max_iter=5,\n",
    "    learning_method=\"batch\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# fitting the LDA model\n",
    "lda_k80.fit(tf)\n",
    "\n",
    "# computing the coherence of the topic model with K=40\n",
    "k80_coh = tmtoolkit.topicmod.evaluate.metric_coherence_gensim(measure='c_v', \n",
    "                        top_n=25, \n",
    "                        topic_word_distrib=lda_k80.components_, # the components of the lda count as \n",
    "                        dtm=tf, # the term frequency\n",
    "                        vocab=np.array([x for x in tfidf_vectorizer.vocabulary_.keys()]), # pass in vectorizer\n",
    "                        texts=df[\"tokens\"].values) # pass in list of tokenized texts -> needs to match vocab\n",
    "\n",
    "# saving the model\n",
    "pickle.dump(lda_k80, open(\"min_act_lda_k80.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e8a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# initializing the LDA model with a k of 100\n",
    "lda_k100 = LatentDirichletAllocation(\n",
    "    n_components=100,\n",
    "    n_jobs=5,\n",
    "    max_iter=5,\n",
    "    learning_method=\"batch\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# fitting the LDA model\n",
    "lda_k100.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f54eb1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the k100 model (b/c it got angry during coherence calculation (memory issues))\n",
    "pickle.dump(lda_k100, open(\"min_act_lda_k100.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6136228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coh_perp_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dab9ca38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 513\n",
      "Perplexity at K=7: 341.13637105754117\n",
      "Coherence at K=7: [0.5123705802925321, 0.5600361551715582, 0.4646701220074818, 0.468973953046122, 0.3668484267436407, 0.4276540135840512, 0.44490309714029197]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashbu\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# setting an entry \n",
    "coh_perp_dict.setdefault(\"k7\", {})\n",
    "\n",
    "# calculating perplexity\n",
    "perplexity = lda_k7.perplexity(tf)\n",
    "\n",
    "# printing perplexity and adding it to the dict\n",
    "print(\"Perplexity at K=7:\", lda_k7.perplexity(tf))\n",
    "coh_perp_dict[\"k7\"].setdefault(\"perplexity\", perplexity)\n",
    "\n",
    "# printing coherence and adding it to the dict\n",
    "print(\"Coherence at K=7:\", k7_coh)\n",
    "coh_perp_dict[\"k7\"].setdefault(\"coherence\", k7_coh)\n",
    "\n",
    "# making the visualization, showing it, and saving it\n",
    "vis = pyLDAvis.sklearn.prepare(lda_k7, tf, tfidf_vectorizer) # lda_tf, dtm, tf_vectorizzer\n",
    "vis\n",
    "pyLDAvis.save_html(vis,\"ldak7_pyldavis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05bda7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity at K=10: 392.6578657869459\n",
      "Coherence at K=10: [0.44928052052107753, 0.5870318763092695, 0.49203942774128706, 0.49017830144810437, 0.43308165395573006, 0.41542065002646283, 0.4913483792609603, 0.5861142703704031, 0.4649887089652231, 0.38188149365058005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashbu\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# setting an entry \n",
    "coh_perp_dict.setdefault(\"k10\", {})\n",
    "\n",
    "# calculating perplexity\n",
    "perplexity = lda_k10.perplexity(tf)\n",
    "\n",
    "# printing perplexity and adding it to the dict\n",
    "print(\"Perplexity at K=10:\", lda_k10.perplexity(tf))\n",
    "coh_perp_dict[\"k10\"].setdefault(\"perplexity\", perplexity)\n",
    "\n",
    "# printing coherence and adding it to the dict\n",
    "print(\"Coherence at K=10:\", k10_coh)\n",
    "coh_perp_dict[\"k10\"].setdefault(\"coherence\", k10_coh)\n",
    "\n",
    "# making the visualization, showing it, and saving it\n",
    "vis = pyLDAvis.sklearn.prepare(lda_k10, tf, tfidf_vectorizer) # lda_tf, dtm, tf_vectorizzer\n",
    "vis\n",
    "pyLDAvis.save_html(vis,\"ldak10_pyldavis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a29c724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity at K=15: 470.128374859617\n",
      "Coherence at K=15: [0.46865903286978716, 0.613997309214653, 0.4793665564626367, 0.48310602884360393, 0.4437977399201305, 0.4823084818565156, 0.5459531381101509, 0.591948106962888, 0.49572340661296466, 0.4014054367674835, 0.4747786926441583, 0.4480175119963062, 0.5602666995340048, 0.46077995844845326, 0.43630945766843554]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashbu\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# setting an entry \n",
    "coh_perp_dict.setdefault(\"k15\", {})\n",
    "\n",
    "# calculating perplexity\n",
    "perplexity = lda_k15.perplexity(tf)\n",
    "\n",
    "# printing perplexity and adding it to the dict\n",
    "print(\"Perplexity at K=15:\", lda_k15.perplexity(tf))\n",
    "coh_perp_dict[\"k15\"].setdefault(\"perplexity\", perplexity)\n",
    "\n",
    "# printing coherence and adding it to the dict\n",
    "print(\"Coherence at K=15:\", k15_coh)\n",
    "coh_perp_dict[\"k15\"].setdefault(\"coherence\", k15_coh)\n",
    "\n",
    "# making the visualization, showing it, and saving it\n",
    "vis = pyLDAvis.sklearn.prepare(lda_k15, tf, tfidf_vectorizer) # lda_tf, dtm, tf_vectorizzer\n",
    "vis\n",
    "pyLDAvis.save_html(vis,\"ldak15_pyldavis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45c13ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity at K=20: 540.505461938051\n",
      "Coherence at K=20: [0.4969279707378436, 0.5341753563672381, 0.5182183487904384, 0.5313740007126243, 0.46252197765086533, 0.48781013258191946, 0.5459543592841053, 0.5798280157293888, 0.5294497140583942, 0.42839021849888753, 0.4801832760326407, 0.46660358945457736, 0.5477684105637548, 0.4840430327035433, 0.4600931041473597, 0.5099425079576099, 0.6012583257634511, 0.5044340836767573, 0.44980744939255324, 0.4369006770715288]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashbu\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# setting an entry \n",
    "coh_perp_dict.setdefault(\"k20\", {})\n",
    "\n",
    "# calculating perplexity\n",
    "perplexity = lda_k20.perplexity(tf)\n",
    "\n",
    "# printing perplexity and adding it to the dict\n",
    "print(\"Perplexity at K=20:\", lda_k20.perplexity(tf))\n",
    "coh_perp_dict[\"k20\"].setdefault(\"perplexity\", perplexity)\n",
    "\n",
    "# printing coherence and adding it to the dict\n",
    "print(\"Coherence at K=20:\", k20_coh)\n",
    "coh_perp_dict[\"k20\"].setdefault(\"coherence\", k20_coh)\n",
    "\n",
    "# making the visualization, showing it, and saving it\n",
    "vis = pyLDAvis.sklearn.prepare(lda_k20, tf, tfidf_vectorizer) # lda_tf, dtm, tf_vectorizzer\n",
    "vis\n",
    "pyLDAvis.save_html(vis,\"ldak20_pyldavis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab9f4ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity at K=25: 612.5863926854026\n",
      "Coherence at K=25: [0.478196674399773, 0.5549206949535486, 0.5061438067011512, 0.5109319113716678, 0.4507465154170339, 0.4839048538401934, 0.5119740367052873, 0.5696034876038543, 0.5180541542790074, 0.5060990283875013, 0.5102516532166843, 0.4545815439779977, 0.5536216428469788, 0.4772068661339369, 0.4560207639399364, 0.5080670263567529, 0.545411978534512, 0.5086707011294922, 0.44334730488175766, 0.455964296619815, 0.4893062459645063, 0.44422081432601485, 0.3930329699628306, 0.5089539493490676, 0.4591217089179801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashbu\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# setting an entry \n",
    "coh_perp_dict.setdefault(\"k25\", {})\n",
    "\n",
    "# calculating perplexity\n",
    "perplexity = lda_k25.perplexity(tf)\n",
    "\n",
    "# printing perplexity and adding it to the dict\n",
    "print(\"Perplexity at K=25:\", lda_k25.perplexity(tf))\n",
    "coh_perp_dict[\"k25\"].setdefault(\"perplexity\", perplexity)\n",
    "\n",
    "# printing coherence and adding it to the dict\n",
    "print(\"Coherence at K=25:\", k25_coh)\n",
    "coh_perp_dict[\"k25\"].setdefault(\"coherence\", k25_coh)\n",
    "\n",
    "# making the visualization, showing it, and saving it\n",
    "vis = pyLDAvis.sklearn.prepare(lda_k25, tf, tfidf_vectorizer) # lda_tf, dtm, tf_vectorizzer\n",
    "vis\n",
    "pyLDAvis.save_html(vis,\"ldak25_pyldavis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "957cc97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity at K=30: 677.4449957087228\n",
      "Coherence at K=30: [0.5401932357827812, 0.5786367078489684, 0.4539459375938849, 0.5090271295188388, 0.4366398931950942, 0.4705215639704069, 0.47889241074610156, 0.5815233429121788, 0.5509941711604067, 0.5295555736431116, 0.4984953554863468, 0.4531463182003114, 0.5322322092556045, 0.46725707561924096, 0.494270260343941, 0.5081427198718346, 0.5477762539854223, 0.5088700057434151, 0.4900989759624763, 0.4687364634275203, 0.5140339512756367, 0.4712210855818806, 0.4495911447379163, 0.5062137419946923, 0.46314711785930135, 0.4436535749571974, 0.4974392828654084, 0.450667953677198, 0.5485455684884136, 0.47164027985045454]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashbu\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# setting an entry \n",
    "coh_perp_dict.setdefault(\"k30\", {})\n",
    "\n",
    "# calculating perplexity\n",
    "perplexity = lda_k30.perplexity(tf)\n",
    "\n",
    "# printing perplexity and adding it to the dict\n",
    "print(\"Perplexity at K=30:\", lda_k30.perplexity(tf))\n",
    "coh_perp_dict[\"k30\"].setdefault(\"perplexity\", perplexity)\n",
    "\n",
    "# printing coherence and adding it to the dict\n",
    "print(\"Coherence at K=30:\", k30_coh)\n",
    "coh_perp_dict[\"k30\"].setdefault(\"coherence\", k30_coh)\n",
    "\n",
    "# making the visualization, showing it, and saving it\n",
    "vis = pyLDAvis.sklearn.prepare(lda_k30, tf, tfidf_vectorizer) # lda_tf, dtm, tf_vectorizzer\n",
    "vis\n",
    "pyLDAvis.save_html(vis,\"ldak30_pyldavis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a23e44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity at K=40: 810.6749566447656\n",
      "Coherence at K=40: [0.5471366068938177, 0.5548199268767771, 0.5202066271294573, 0.5026316078255137, 0.4368206038744746, 0.4920161643929495, 0.43716617441229166, 0.5238645938949315, 0.5299055826501491, 0.5230874557760254, 0.4865080020785869, 0.4637658927819328, 0.5237638609782623, 0.4860576398248645, 0.4681595760324324, 0.48751376693235604, 0.48128570054949416, 0.500780459652435, 0.5511830100515026, 0.4608143797318885, 0.48248344046380076, 0.47449687753168224, 0.45164174198517915, 0.5228103963361, 0.4423257991413454, 0.38588082947256164, 0.4689329350704455, 0.459461727573691, 0.48875821229094935, 0.5081889600708951, 0.39628704458183495, 0.4778592028471815, 0.44698125876946193, 0.5485915132688041, 0.48831657498968734, 0.5257843495363052, 0.5081253398769212, 0.46664981635144837, 0.5194490965954935, 0.46750573307113197]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashbu\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# setting an entry \n",
    "coh_perp_dict.setdefault(\"k40\", {})\n",
    "\n",
    "# calculating perplexity\n",
    "perplexity = lda_k40.perplexity(tf)\n",
    "\n",
    "# printing perplexity and adding it to the dict\n",
    "print(\"Perplexity at K=40:\", lda_k40.perplexity(tf))\n",
    "coh_perp_dict[\"k40\"].setdefault(\"perplexity\", perplexity)\n",
    "\n",
    "# printing coherence and adding it to the dict\n",
    "print(\"Coherence at K=40:\", k40_coh)\n",
    "coh_perp_dict[\"k40\"].setdefault(\"coherence\", k40_coh)\n",
    "\n",
    "# making the visualization, showing it, and saving it\n",
    "vis = pyLDAvis.sklearn.prepare(lda_k40, tf, tfidf_vectorizer) # lda_tf, dtm, tf_vectorizzer\n",
    "vis\n",
    "pyLDAvis.save_html(vis,\"ldak40_pyldavis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98503ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity at K=60: 1298.8439602757503\n",
      "Coherence at K=60: [0.5057010537108281, 0.5617596414362902, 0.517156370104176, 0.438832620771859, 0.4754873194599364, 0.4631811104711372, 0.47641826358223127, 0.4713946630480136, 0.5219099105730491, 0.47377572565380904, 0.46950629775027686, 0.4328049307522603, 0.523592511281315, 0.41564726852890593, 0.5184580874457184, 0.48687358963552363, 0.4843653152255246, 0.5049331390876517, 0.5509038225654639, 0.44869546699138807, 0.4752604763243397, 0.48797075245415744, 0.4605723157874266, 0.4872818742126596, 0.47200937909510426, 0.35640146142473833, 0.455840483803993, 0.4657936518206497, 0.5057906098632887, 0.5005854755729278, 0.4399209659793219, 0.4562847493991713, 0.46022547842477735, 0.5400764169539944, 0.5465594903248954, 0.4907484024893177, 0.5369420168267967, 0.46300338869379154, 0.5368163333892134, 0.4739900378209293, 0.434904939568166, 0.43890601991973294, 0.49222366384600946, 0.47063588776053467, 0.48860344264361577, 0.5272595981029136, 0.48098742213944784, 0.5173656520023908, 0.5391210287332548, 0.4344272100276892, 0.48229164783518474, 0.43710848989020396, 0.5282366800755478, 0.5177794221195247, 0.4749940436638262, 0.48037551723081406, 0.5452241934748565, 0.5892258358571599, 0.5852022658134874, 0.4757712534368113]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashbu\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# setting an entry \n",
    "coh_perp_dict.setdefault(\"k60\", {})\n",
    "\n",
    "# calculating perplexity\n",
    "perplexity = lda_k60.perplexity(tf)\n",
    "\n",
    "# printing perplexity and adding it to the dict\n",
    "print(\"Perplexity at K=60:\", lda_k60.perplexity(tf))\n",
    "coh_perp_dict[\"k60\"].setdefault(\"perplexity\", perplexity)\n",
    "\n",
    "# printing coherence and adding it to the dict\n",
    "print(\"Coherence at K=60:\", k60_coh)\n",
    "coh_perp_dict[\"k60\"].setdefault(\"coherence\", k60_coh)\n",
    "\n",
    "# making the visualization, showing it, and saving it\n",
    "vis = pyLDAvis.sklearn.prepare(lda_k60, tf, tfidf_vectorizer) # lda_tf, dtm, tf_vectorizzer\n",
    "vis\n",
    "pyLDAvis.save_html(vis,\"ldak60_pyldavis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b135c304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity at K=80: 2047.6577626188505\n",
      "Coherence at K=80: [0.4753756746661836, 0.4980040698047585, 0.5122364145664362, 0.5045920803295105, 0.4506504578082796, 0.46767861981929737, 0.4398780428718886, 0.4915194288362925, 0.4789040775539305, 0.49773972981177544, 0.47114495218928676, 0.44553256019320847, 0.5014364015415063, 0.42873270359326704, 0.5257358480062522, 0.47160329988908706, 0.48155967135121314, 0.5377457844326579, 0.545692383546016, 0.4451079727359365, 0.443741931386742, 0.45913829353867686, 0.45135707775001743, 0.48075337621814535, 0.4670324877067422, 0.4017590796678162, 0.44021294308500625, 0.4808317425804212, 0.469771421473088, 0.501654032580548, 0.4305365636460669, 0.5308264657945282, 0.48594571184664453, 0.5871476821353165, 0.4898984321441649, 0.5110286762951533, 0.45847007664908795, 0.47228178550645994, 0.48780072930573626, 0.48623450800925916, 0.47821690255639293, 0.4388618065533912, 0.4992818341395847, 0.4962783780310206, 0.47820072823799, 0.4652862252454952, 0.48251248047174977, 0.47978915269213324, 0.4932180924300087, 0.44903155506304016, 0.41738132229180513, 0.4601208932331001, 0.5575332492799564, 0.5520617169462033, 0.4609772413505101, 0.5411467684361133, 0.48807636043220004, 0.5903694905392406, 0.6154014826443028, 0.47003568795395667, 0.4805478638617752, 0.45823108514232047, 0.4537261992144982, 0.4686804195484126, 0.48314658247538206, 0.42588714117863014, 0.4719891921150712, 0.48019667084714035, 0.462268000950814, 0.49757878402448724, 0.5585766602104589, 0.5287351314586888, 0.48591379876911034, 0.4885815029048731, 0.5608355531553206, 0.5529623951758499, 0.5195078274661424, 0.5605598141524876, 0.48593358754170124, 0.45592659919778156]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashbu\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "# setting an entry \n",
    "coh_perp_dict.setdefault(\"k80\", {})\n",
    "\n",
    "# calculating perplexity\n",
    "perplexity = lda_k80.perplexity(tf)\n",
    "\n",
    "# printing perplexity and adding it to the dict\n",
    "print(\"Perplexity at K=80:\", lda_k80.perplexity(tf))\n",
    "coh_perp_dict[\"k80\"].setdefault(\"perplexity\", perplexity)\n",
    "\n",
    "# printing coherence and adding it to the dict\n",
    "print(\"Coherence at K=80:\", k80_coh)\n",
    "coh_perp_dict[\"k80\"].setdefault(\"coherence\", k80_coh)\n",
    "\n",
    "# making the visualization, showing it, and saving it\n",
    "vis = pyLDAvis.sklearn.prepare(lda_k80, tf, tfidf_vectorizer) # lda_tf, dtm, tf_vectorizzer\n",
    "vis\n",
    "pyLDAvis.save_html(vis,\"ldak80_pyldavis.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing the dict\n",
    "# saving the the test run coherence and perplexity for each of the models\n",
    "with open('coh_perp.json', 'w') as outfile:\n",
    "    json.dump(coh_perp_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea10802",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc4576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dddb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ceaea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a17225c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
