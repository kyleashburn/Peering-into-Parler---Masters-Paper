{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c570293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fasttext\n",
    "from pycountry import languages\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "989cace4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_PATH = 'lid.176.bin'\n",
    "model = fasttext.load_model(PRETRAINED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2de2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to help figure out the language\n",
    "# adjusted to check if the body is a float (nan)\n",
    "def lang_class(x):\n",
    "    # little bit of error handling in case the darn thing isn't working the way I'd like\n",
    "    if type(x) != float:\n",
    "        x = x .replace(\"\\n\", \" \")\n",
    "        lang = model.predict(x)\n",
    "        \n",
    "        return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f7f742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to help retrieve the language name using the language code\n",
    "def lang_from_code(x):\n",
    "    x = x[0]\n",
    "    x = str(x)\n",
    "  \n",
    "    match = re.match(r\"__label__([a-zA-Z]+)\", x)\n",
    "    match = match.group(1)\n",
    "\n",
    "    if len(match) == 2:\n",
    "        \n",
    "        try:\n",
    "            lang = languages.get(alpha_2=match).name\n",
    "\n",
    "        except:\n",
    "            lang = match\n",
    "    elif len(match) == 3:\n",
    "        try:\n",
    "            lang = languages.get(alpha_3=match).name\n",
    "        \n",
    "        except:   \n",
    "            lang = match\n",
    "            \n",
    "        \n",
    "    return(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8106194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to detect the languages, apply those as labels, and then subset on that\n",
    "# function gets passed the whole dataframe\n",
    "def lang_filt(df):\n",
    "    # making the lists of cols to drop & keep\n",
    "    to_keep = [\"t\",\"id\",\"cd\",\"c\",\"u\",\"un\", \"dmn\", \"bo\"]\n",
    "    to_drop = [x for x in list(df.columns) if x not in to_keep]\n",
    "        \n",
    "    # dropping the cols\n",
    "    df = df.drop(to_drop, axis = 1)\n",
    "     \n",
    "     # filtering to those which have a body (and can have a language classifier run on them)\n",
    "    df = df[df[\"bo\"].notna()]\n",
    "    \n",
    "    # applying the lang_class func\n",
    "    df[\"bo_lang\"] = df[\"bo\"].apply(lang_class)\n",
    "       \n",
    "    # making the list of labels and confidences\n",
    "    df[[\"bo_lang\",\"conf\"]] = pd.DataFrame(df[df[\"bo_lang\"].notna()][\"bo_lang\"].tolist(), index=df[df[\"bo\"].notna()].index)\n",
    "    \n",
    "    # pulling the language codes\n",
    "    df[\"bo_lang\"] = df[df[\"bo_lang\"].notna()][\"bo_lang\"].apply(lang_from_code)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d78cddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kinda messy -> can clean this up so it's prettier\n",
    "# function to pull out the top languages in there before filtering\n",
    "# function gets passed whole dataframe\n",
    "def top_langs(df, lang_dict, top_n):    \n",
    "    \n",
    "    # starting with an empty dict for overall\n",
    "    lang_dict.setdefault(\"overall\", {})\n",
    "                \n",
    "    # following up by converting counts and percents to a dictionary & appending those\n",
    "    lang_dict[\"overall\"].setdefault(\"counts\",\n",
    "                                df[df[\"bo_lang\"].notna()][\"bo_lang\"].value_counts().head(top_n).to_dict())\n",
    "    \n",
    "    lang_dict[\"overall\"].setdefault(\"percents\",\n",
    "                                df[df[\"bo_lang\"].notna()][\"bo_lang\"].value_counts(normalize=True).head(top_n).to_dict())\n",
    "\n",
    "    # starting with an empty dict for the posts\n",
    "    lang_dict.setdefault(\"posts\", {})\n",
    "        \n",
    "    # following up by converting counts and percents to a dictionary & appending those\n",
    "    lang_dict[\"posts\"].setdefault(\"counts\",\n",
    "                        df[(df[\"bo_lang\"].notna()) & (df[\"t\"] == \"p\")][\"bo_lang\"].value_counts().head(top_n).to_dict())\n",
    "    \n",
    "    lang_dict[\"posts\"].setdefault(\"percents\",\n",
    "        df[(df[\"bo_lang\"].notna()) & (df[\"t\"] == \"p\")][\"bo_lang\"].value_counts(normalize=True).head(top_n).to_dict())\n",
    "\n",
    "    # starting with an empty dict for the comments\n",
    "    lang_dict.setdefault(\"comments\", {})\n",
    "        \n",
    "    # following up by converting counts and percents to a dictionary & appending those\n",
    "    lang_dict[\"comments\"].setdefault(\"counts\",\n",
    "                        df[(df[\"bo_lang\"].notna()) & (df[\"t\"] == \"c\")][\"bo_lang\"].value_counts().head(top_n).to_dict())\n",
    "    \n",
    "    lang_dict[\"comments\"].setdefault(\"percents\",\n",
    "        df[(df[\"bo_lang\"].notna()) & (df[\"t\"] == \"c\")][\"bo_lang\"].value_counts(normalize=True).head(top_n).to_dict())\n",
    "    \n",
    "    # returns the lagnuage dict we passed it with those adjustments\n",
    "    return lang_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9b1dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes in file name and gives a new one for the new save file we're making\n",
    "def file_name(file):\n",
    "    # splitting on \".\"\n",
    "    file = file.split(\".\")\n",
    "    # adding the name to show it's been filtered\n",
    "    file[0] = file[0] + \"_lang_filtered\"\n",
    "    # ading the file type\n",
    "    file = file[0] + \".\" + file[1]\n",
    "    # returning the file\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79304a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated code\n",
    "# print(\"\\nOverall\")\n",
    "# # printing the counts and percents overall\n",
    "# print(pd.concat([df[df[\"bo_lang\"].notna()][\"bo_lang\"].value_counts().head(top_n), \n",
    "#       df[df[\"bo_lang\"].notna()][\"bo_lang\"].value_counts(normalize=True).head(top_n)], axis=1, keys=[\"count\", \"pct\"]))\n",
    "\n",
    "# print(\"\\nPosts\")\n",
    "# # printing counts & percents for posts\n",
    "# print(pd.concat([df[(df[\"bo_lang\"].notna()) & (df[\"t\"] == \"p\")][\"bo_lang\"].value_counts().head(top_n), \n",
    "#      df[(df[\"bo_lang\"].notna()) & (df[\"t\"] == \"p\")][\"bo_lang\"].value_counts(normalize=True).head(top_n)],\n",
    "#         axis=1, keys=[\"count\", \"pct\"]))\n",
    "\n",
    "# print(\"\\nComments\")\n",
    "# # printing counts & percents for comments\n",
    "# print(pd.concat([df[(df[\"bo_lang\"].notna()) & (df[\"t\"] == \"c\")][\"bo_lang\"].value_counts().head(top_n), \n",
    "#      df[(df[\"bo_lang\"].notna()) & (df[\"t\"] == \"c\")][\"bo_lang\"].value_counts(normalize=True).head(top_n)],\n",
    "#         axis=1, keys=[\"count\", \"pct\"]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# still need to change the lange filtering to follow the same approach I used in the domain filtering\n",
    "# to whit, I need to output top n results to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a9d863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this is a test!!!\n",
    "# this is meant to output a dictionary that I can then turn into an ndjson file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8f16618",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coh_perp_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:38\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'coh_perp_dict' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# needs to run through each of the datasets -> they need to be read in\n",
    "lang_dict = {}\n",
    "\n",
    "for file in file_list:\n",
    "    # reading file   \n",
    "    df = pd.read_json(file, lines=True)\n",
    "    \n",
    "           \n",
    "    # filtering to english language only & pulling the languages prior to filtering \n",
    "    # (those languages are post empty bo rm)\n",
    "    df = lang_filt(df)\n",
    "    \n",
    "    # declaring empty dict\n",
    "    lang_dict.setdefault(file, {})\n",
    "\n",
    "    # running on the top 10 languages\n",
    "    lang_dict[file] = top_langs(df, lang_dict[file], 10)\n",
    "\n",
    "    \n",
    "    # filtering the language down to just english\n",
    "    df = df[df[\"bo_lang\"] == \"English\"]\n",
    "    \n",
    "    # possible\n",
    "    \n",
    "    \n",
    "    # creating the file name\n",
    "    filename = file_name(file)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # exporting the file\n",
    "    df.to_json(filename, orient=\"records\",lines=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7017de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializing the lang_dict\n",
    "# saving the the test run coherence and perplexity for each of the models\n",
    "with open('lang_json', 'w') as outfile:\n",
    "    json.dump(lang_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26569a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b23cc1f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>id</th>\n",
       "      <th>cd</th>\n",
       "      <th>c</th>\n",
       "      <th>u</th>\n",
       "      <th>un</th>\n",
       "      <th>dmn</th>\n",
       "      <th>bo</th>\n",
       "      <th>bo_lang</th>\n",
       "      <th>conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p</td>\n",
       "      <td>3619348b91524430882f2b887838a3e4</td>\n",
       "      <td>20201018</td>\n",
       "      <td>1602989910000</td>\n",
       "      <td>8b67993183a14587a001010058d089d2</td>\n",
       "      <td>chucknellis</td>\n",
       "      <td>noqreport.com</td>\n",
       "      <td>Facebook’s Public Policy Manager for Global El...</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.8107684254646301]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p</td>\n",
       "      <td>1365051bfe6243599e6af1055b71c4a2</td>\n",
       "      <td>20201202</td>\n",
       "      <td>1606897009000</td>\n",
       "      <td>eeeb8dd25b7142b1bc69cbdbe1d8bb62</td>\n",
       "      <td>ThomasFox</td>\n",
       "      <td>rumble.com</td>\n",
       "      <td>USPS Driver Drops SHOCKING Claim About 200,000...</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.6809220314025879]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>p</td>\n",
       "      <td>d755c2ce09e74c9b9b34953aae839554</td>\n",
       "      <td>20190829</td>\n",
       "      <td>1567070618000</td>\n",
       "      <td>21031f424913456591d9a9aed4ff26c7</td>\n",
       "      <td>Cobrarick98</td>\n",
       "      <td>i.imgur.com</td>\n",
       "      <td>Antifa. You touch a Trump supporter we will be...</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.8768866658210754]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>p</td>\n",
       "      <td>a0124e3f9afa4fe29d404e41e3ac9a5f</td>\n",
       "      <td>20201009</td>\n",
       "      <td>1602262587000</td>\n",
       "      <td>6e6d4f8e7479446f8f06d5d5c0fae9a3</td>\n",
       "      <td>AppleJax</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>If the guilty aren't charged before the electi...</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.9819905757904053]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>p</td>\n",
       "      <td>857c1349a1cc4bfba84cb75a9a34ec93</td>\n",
       "      <td>20201026</td>\n",
       "      <td>1603746735000</td>\n",
       "      <td>eeeb8dd25b7142b1bc69cbdbe1d8bb62</td>\n",
       "      <td>ThomasFox</td>\n",
       "      <td>deadline.com</td>\n",
       "      <td>‘60 Minutes’ &amp; Trump Walk-Off Soars To 17M Vie...</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.7728871703147888]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   t                                id        cd              c  \\\n",
       "1  p  3619348b91524430882f2b887838a3e4  20201018  1602989910000   \n",
       "2  p  1365051bfe6243599e6af1055b71c4a2  20201202  1606897009000   \n",
       "5  p  d755c2ce09e74c9b9b34953aae839554  20190829  1567070618000   \n",
       "7  p  a0124e3f9afa4fe29d404e41e3ac9a5f  20201009  1602262587000   \n",
       "9  p  857c1349a1cc4bfba84cb75a9a34ec93  20201026  1603746735000   \n",
       "\n",
       "                                  u           un                   dmn  \\\n",
       "1  8b67993183a14587a001010058d089d2  chucknellis         noqreport.com   \n",
       "2  eeeb8dd25b7142b1bc69cbdbe1d8bb62    ThomasFox            rumble.com   \n",
       "5  21031f424913456591d9a9aed4ff26c7  Cobrarick98           i.imgur.com   \n",
       "7  6e6d4f8e7479446f8f06d5d5c0fae9a3     AppleJax  image-cdn.parler.com   \n",
       "9  eeeb8dd25b7142b1bc69cbdbe1d8bb62    ThomasFox          deadline.com   \n",
       "\n",
       "                                                  bo  bo_lang  \\\n",
       "1  Facebook’s Public Policy Manager for Global El...  English   \n",
       "2  USPS Driver Drops SHOCKING Claim About 200,000...  English   \n",
       "5  Antifa. You touch a Trump supporter we will be...  English   \n",
       "7  If the guilty aren't charged before the electi...  English   \n",
       "9  ‘60 Minutes’ & Trump Walk-Off Soars To 17M Vie...  English   \n",
       "\n",
       "                   conf  \n",
       "1  [0.8107684254646301]  \n",
       "2  [0.6809220314025879]  \n",
       "5  [0.8768866658210754]  \n",
       "7  [0.9819905757904053]  \n",
       "9  [0.7728871703147888]  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140e8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e61948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0409965",
   "metadata": {},
   "source": [
    "#### Detecting the languages and adding that as a class filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1146e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [\"min_act.ndjson\", \"mod_act.ndjson\", \"ver_act.ndjson\",\"ext_act.ndjson\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae015ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_act_lang_filtered.ndjson\n",
      "mod_act_lang_filtered.ndjson\n",
      "ver_act_lang_filtered.ndjson\n",
      "ext_act_lang_filtered.ndjson\n"
     ]
    }
   ],
   "source": [
    "for file in file_list:\n",
    "    file = file.split(\".\")\n",
    "    file[0] = file[0] + \"_lang_filtered\"\n",
    "    file = file[0] + \".\" + file[1]\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf3953bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dataset is: min_act.ndjson\n",
      "Language information for file:\n",
      "\n",
      "Overall\n",
      "               count       pct\n",
      "English     21506207  0.911404\n",
      "Portuguese    332572  0.014094\n",
      "Arabic        250503  0.010616\n",
      "German        223956  0.009491\n",
      "Spanish       178334  0.007558\n",
      "Japanese      177927  0.007540\n",
      "Russian       114725  0.004862\n",
      "Chinese       109535  0.004642\n",
      "Italian       103426  0.004383\n",
      "French         88103  0.003734\n",
      "\n",
      "Posts\n",
      "              count       pct\n",
      "English     5319874  0.912323\n",
      "Arabic       124498  0.021351\n",
      "Portuguese   106613  0.018283\n",
      "Spanish       42037  0.007209\n",
      "German        29656  0.005086\n",
      "Japanese      29480  0.005056\n",
      "Persian       28209  0.004838\n",
      "Chinese       18808  0.003225\n",
      "French        18007  0.003088\n",
      "Italian       17841  0.003060\n",
      "\n",
      "Comments\n",
      "               count       pct\n",
      "English     16186333  0.911103\n",
      "Portuguese    225959  0.012719\n",
      "German        194300  0.010937\n",
      "Japanese      148447  0.008356\n",
      "Spanish       136297  0.007672\n",
      "Arabic        126005  0.007093\n",
      "Russian       102958  0.005795\n",
      "Chinese        90727  0.005107\n",
      "Italian        85585  0.004817\n",
      "French         70096  0.003946\n",
      "Current Dataset is: mod_act.ndjson\n",
      "Language information for file:\n",
      "\n",
      "Overall\n",
      "               count       pct\n",
      "English     14001320  0.898585\n",
      "Arabic        380898  0.024445\n",
      "Persian       189183  0.012141\n",
      "Portuguese    188758  0.012114\n",
      "Japanese      103251  0.006627\n",
      "German         93761  0.006017\n",
      "Spanish        87835  0.005637\n",
      "Chinese        69300  0.004448\n",
      "Russian        61412  0.003941\n",
      "French         56936  0.003654\n",
      "\n",
      "Posts\n",
      "                   count       pct\n",
      "English          3288283  0.862955\n",
      "Arabic            121264  0.031824\n",
      "Portuguese        116310  0.030524\n",
      "Persian            46621  0.012235\n",
      "Japanese           37622  0.009873\n",
      "Spanish            34077  0.008943\n",
      "Chinese            26655  0.006995\n",
      "French             19295  0.005064\n",
      "German             19128  0.005020\n",
      "Egyptian Arabic    15900  0.004173\n",
      "\n",
      "Comments\n",
      "                    count       pct\n",
      "English          10713037  0.910119\n",
      "Arabic             259634  0.022057\n",
      "Persian            142562  0.012111\n",
      "German              74633  0.006340\n",
      "Portuguese          72448  0.006155\n",
      "Japanese            65629  0.005575\n",
      "Spanish             53758  0.004567\n",
      "Russian             50050  0.004252\n",
      "Chinese             42645  0.003623\n",
      "Egyptian Arabic     39959  0.003395\n",
      "Current Dataset is: ver_act.ndjson\n",
      "Language information for file:\n",
      "\n",
      "Overall\n",
      "                   count       pct\n",
      "English          8081465  0.902805\n",
      "Arabic            220350  0.024616\n",
      "Portuguese        102114  0.011407\n",
      "Japanese           77224  0.008627\n",
      "Persian            71841  0.008026\n",
      "German             51502  0.005753\n",
      "Egyptian Arabic    49907  0.005575\n",
      "Chinese            45056  0.005033\n",
      "Spanish            44951  0.005022\n",
      "French             37340  0.004171\n",
      "\n",
      "Posts\n",
      "                   count       pct\n",
      "English          3197384  0.896700\n",
      "Portuguese         72719  0.020394\n",
      "Arabic             70898  0.019883\n",
      "Japanese           43629  0.012236\n",
      "Chinese            24287  0.006811\n",
      "Spanish            23831  0.006683\n",
      "French             20710  0.005808\n",
      "German             19155  0.005372\n",
      "Persian            18385  0.005156\n",
      "Egyptian Arabic    12688  0.003558\n",
      "\n",
      "Comments\n",
      "                   count       pct\n",
      "English          4884081  0.906847\n",
      "Arabic            149452  0.027749\n",
      "Persian            53456  0.009925\n",
      "Egyptian Arabic    37219  0.006911\n",
      "Japanese           33595  0.006238\n",
      "German             32347  0.006006\n",
      "Portuguese         29395  0.005458\n",
      "Spanish            21120  0.003921\n",
      "Chinese            20769  0.003856\n",
      "Russian            19575  0.003635\n",
      "Current Dataset is: ext_act.ndjson\n",
      "Language information for file:\n",
      "\n",
      "Overall\n",
      "              count       pct\n",
      "English     1266682  0.916547\n",
      "Portuguese    25279  0.018291\n",
      "Japanese      21815  0.015785\n",
      "Spanish       12620  0.009132\n",
      "German        10262  0.007425\n",
      "Chinese        8512  0.006159\n",
      "French         6606  0.004780\n",
      "Catalan        4367  0.003160\n",
      "Russian        4361  0.003156\n",
      "Italian        3756  0.002718\n",
      "\n",
      "Posts\n",
      "              count       pct\n",
      "English     1061052  0.933251\n",
      "Portuguese    24135  0.021228\n",
      "Japanese      13695  0.012045\n",
      "Spanish        7227  0.006357\n",
      "German         5597  0.004923\n",
      "Chinese        5029  0.004423\n",
      "French         3852  0.003388\n",
      "Catalan        3815  0.003355\n",
      "Russian        2231  0.001962\n",
      "Italian        1557  0.001369\n",
      "\n",
      "Comments\n",
      "             count       pct\n",
      "English     205630  0.839056\n",
      "Japanese      8120  0.033133\n",
      "Spanish       5393  0.022006\n",
      "German        4665  0.019035\n",
      "Chinese       3483  0.014212\n",
      "French        2754  0.011237\n",
      "Italian       2199  0.008973\n",
      "Russian       2130  0.008691\n",
      "Portuguese    1144  0.004668\n",
      "Esperanto      938  0.003827\n",
      "CPU times: total: 36min 39s\n",
      "Wall time: 37min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for file in file_list:\n",
    "    # reading file\n",
    "    df = pd.read_json(file, lines=True)\n",
    "    \n",
    "    # printing out the current dataset we're working on\n",
    "    print(\"Current Dataset is:\", file)\n",
    "    print(\"Language information for file:\")\n",
    "    \n",
    "    # filtering to english language only\n",
    "    df = lang_filt(df)\n",
    "    \n",
    "    filename = file_name(file)\n",
    "    \n",
    "    # exporting the file\n",
    "    df.to_json(filename, orient=\"records\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f674a0be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 25.2 s\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_json(\"ext_act.ndjson\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f8c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1) rm those without bodeies\n",
    "# step 2) get lang_counts\n",
    "# step 3) filter eng only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25fd7166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>id</th>\n",
       "      <th>cd</th>\n",
       "      <th>c</th>\n",
       "      <th>u</th>\n",
       "      <th>un</th>\n",
       "      <th>dmn</th>\n",
       "      <th>bo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p</td>\n",
       "      <td>3619348b91524430882f2b887838a3e4</td>\n",
       "      <td>20201018</td>\n",
       "      <td>1602989910000</td>\n",
       "      <td>8b67993183a14587a001010058d089d2</td>\n",
       "      <td>chucknellis</td>\n",
       "      <td>noqreport.com</td>\n",
       "      <td>Facebook’s Public Policy Manager for Global El...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p</td>\n",
       "      <td>1365051bfe6243599e6af1055b71c4a2</td>\n",
       "      <td>20201202</td>\n",
       "      <td>1606897009000</td>\n",
       "      <td>eeeb8dd25b7142b1bc69cbdbe1d8bb62</td>\n",
       "      <td>ThomasFox</td>\n",
       "      <td>rumble.com</td>\n",
       "      <td>USPS Driver Drops SHOCKING Claim About 200,000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>p</td>\n",
       "      <td>d755c2ce09e74c9b9b34953aae839554</td>\n",
       "      <td>20190829</td>\n",
       "      <td>1567070618000</td>\n",
       "      <td>21031f424913456591d9a9aed4ff26c7</td>\n",
       "      <td>Cobrarick98</td>\n",
       "      <td>i.imgur.com</td>\n",
       "      <td>Antifa. You touch a Trump supporter we will be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>p</td>\n",
       "      <td>a0124e3f9afa4fe29d404e41e3ac9a5f</td>\n",
       "      <td>20201009</td>\n",
       "      <td>1602262587000</td>\n",
       "      <td>6e6d4f8e7479446f8f06d5d5c0fae9a3</td>\n",
       "      <td>AppleJax</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>If the guilty aren't charged before the electi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>p</td>\n",
       "      <td>857c1349a1cc4bfba84cb75a9a34ec93</td>\n",
       "      <td>20201026</td>\n",
       "      <td>1603746735000</td>\n",
       "      <td>eeeb8dd25b7142b1bc69cbdbe1d8bb62</td>\n",
       "      <td>ThomasFox</td>\n",
       "      <td>deadline.com</td>\n",
       "      <td>‘60 Minutes’ &amp; Trump Walk-Off Soars To 17M Vie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254410</th>\n",
       "      <td>c</td>\n",
       "      <td>ae0a1d21bc7144a7a024c90dcf7c1a5d</td>\n",
       "      <td>20200723</td>\n",
       "      <td>1595479941000</td>\n",
       "      <td>f8b37b14d52f493eab60c60486d5e6c6</td>\n",
       "      <td>Aj118</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>@Salonsbocaraton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254416</th>\n",
       "      <td>c</td>\n",
       "      <td>dc3fa8b7c12c4e3c898af4b14c99cbf5</td>\n",
       "      <td>20200727</td>\n",
       "      <td>1595810868000</td>\n",
       "      <td>6e6d4f8e7479446f8f06d5d5c0fae9a3</td>\n",
       "      <td>AppleJax</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>Yeah I know it’s bullshit but it’s also racist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254419</th>\n",
       "      <td>c</td>\n",
       "      <td>f2b26de4201e4519879b63d4b1e0eea7</td>\n",
       "      <td>20200824</td>\n",
       "      <td>1598305363000</td>\n",
       "      <td>f8b37b14d52f493eab60c60486d5e6c6</td>\n",
       "      <td>Aj118</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>@Buellh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254423</th>\n",
       "      <td>c</td>\n",
       "      <td>1dc1abde35234e32a872bcf76bd6ab45</td>\n",
       "      <td>20200820</td>\n",
       "      <td>1597891420000</td>\n",
       "      <td>efaac1ece6a893e0fab48f558e3a5c57</td>\n",
       "      <td>BethocAeilflaed</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>This…\\n\\nFull of wine…\\n\\nOnly thing keeping m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254425</th>\n",
       "      <td>c</td>\n",
       "      <td>476d74eb1d2b4206b72cb02574c105a7</td>\n",
       "      <td>20200928</td>\n",
       "      <td>1601317643000</td>\n",
       "      <td>9ba302eb09584878ae72c873d550eb11</td>\n",
       "      <td>doutingthomas1</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>She’s going to hook them up to her Hoyer lift ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1382015 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         t                                id        cd              c  \\\n",
       "1        p  3619348b91524430882f2b887838a3e4  20201018  1602989910000   \n",
       "2        p  1365051bfe6243599e6af1055b71c4a2  20201202  1606897009000   \n",
       "5        p  d755c2ce09e74c9b9b34953aae839554  20190829  1567070618000   \n",
       "7        p  a0124e3f9afa4fe29d404e41e3ac9a5f  20201009  1602262587000   \n",
       "9        p  857c1349a1cc4bfba84cb75a9a34ec93  20201026  1603746735000   \n",
       "...     ..                               ...       ...            ...   \n",
       "2254410  c  ae0a1d21bc7144a7a024c90dcf7c1a5d  20200723  1595479941000   \n",
       "2254416  c  dc3fa8b7c12c4e3c898af4b14c99cbf5  20200727  1595810868000   \n",
       "2254419  c  f2b26de4201e4519879b63d4b1e0eea7  20200824  1598305363000   \n",
       "2254423  c  1dc1abde35234e32a872bcf76bd6ab45  20200820  1597891420000   \n",
       "2254425  c  476d74eb1d2b4206b72cb02574c105a7  20200928  1601317643000   \n",
       "\n",
       "                                        u               un  \\\n",
       "1        8b67993183a14587a001010058d089d2      chucknellis   \n",
       "2        eeeb8dd25b7142b1bc69cbdbe1d8bb62        ThomasFox   \n",
       "5        21031f424913456591d9a9aed4ff26c7      Cobrarick98   \n",
       "7        6e6d4f8e7479446f8f06d5d5c0fae9a3         AppleJax   \n",
       "9        eeeb8dd25b7142b1bc69cbdbe1d8bb62        ThomasFox   \n",
       "...                                   ...              ...   \n",
       "2254410  f8b37b14d52f493eab60c60486d5e6c6            Aj118   \n",
       "2254416  6e6d4f8e7479446f8f06d5d5c0fae9a3         AppleJax   \n",
       "2254419  f8b37b14d52f493eab60c60486d5e6c6            Aj118   \n",
       "2254423  efaac1ece6a893e0fab48f558e3a5c57  BethocAeilflaed   \n",
       "2254425  9ba302eb09584878ae72c873d550eb11   doutingthomas1   \n",
       "\n",
       "                          dmn  \\\n",
       "1               noqreport.com   \n",
       "2                  rumble.com   \n",
       "5                 i.imgur.com   \n",
       "7        image-cdn.parler.com   \n",
       "9                deadline.com   \n",
       "...                       ...   \n",
       "2254410  image-cdn.parler.com   \n",
       "2254416  image-cdn.parler.com   \n",
       "2254419  image-cdn.parler.com   \n",
       "2254423  image-cdn.parler.com   \n",
       "2254425  image-cdn.parler.com   \n",
       "\n",
       "                                                        bo  \n",
       "1        Facebook’s Public Policy Manager for Global El...  \n",
       "2        USPS Driver Drops SHOCKING Claim About 200,000...  \n",
       "5        Antifa. You touch a Trump supporter we will be...  \n",
       "7        If the guilty aren't charged before the electi...  \n",
       "9        ‘60 Minutes’ & Trump Walk-Off Soars To 17M Vie...  \n",
       "...                                                    ...  \n",
       "2254410                                   @Salonsbocaraton  \n",
       "2254416  Yeah I know it’s bullshit but it’s also racist...  \n",
       "2254419                                            @Buellh  \n",
       "2254423  This…\\n\\nFull of wine…\\n\\nOnly thing keeping m...  \n",
       "2254425  She’s going to hook them up to her Hoyer lift ...  \n",
       "\n",
       "[1382015 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f565db9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "58e03179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Overall\n",
      "              count       pct\n",
      "English     1266682  0.916547\n",
      "Portuguese    25279  0.018291\n",
      "Japanese      21815  0.015785\n",
      "Spanish       12620  0.009132\n",
      "German        10262  0.007425\n",
      "Chinese        8512  0.006159\n",
      "French         6606  0.004780\n",
      "Catalan        4367  0.003160\n",
      "Russian        4361  0.003156\n",
      "Italian        3756  0.002718\n",
      "\n",
      "Posts\n",
      "              count       pct\n",
      "English     1061052  0.933251\n",
      "Portuguese    24135  0.021228\n",
      "Japanese      13695  0.012045\n",
      "Spanish        7227  0.006357\n",
      "German         5597  0.004923\n",
      "Chinese        5029  0.004423\n",
      "French         3852  0.003388\n",
      "Catalan        3815  0.003355\n",
      "Russian        2231  0.001962\n",
      "Italian        1557  0.001369\n",
      "\n",
      "Comments\n",
      "             count       pct\n",
      "English     205630  0.839056\n",
      "Japanese      8120  0.033133\n",
      "Spanish       5393  0.022006\n",
      "German        4665  0.019035\n",
      "Chinese       3483  0.014212\n",
      "French        2754  0.011237\n",
      "Italian       2199  0.008973\n",
      "Russian       2130  0.008691\n",
      "Portuguese    1144  0.004668\n",
      "Esperanto      938  0.003827\n",
      "CPU times: total: 52.5 s\n",
      "Wall time: 52.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = lang_filt(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "75e1f5b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>id</th>\n",
       "      <th>cd</th>\n",
       "      <th>c</th>\n",
       "      <th>u</th>\n",
       "      <th>un</th>\n",
       "      <th>dmn</th>\n",
       "      <th>prov</th>\n",
       "      <th>bo</th>\n",
       "      <th>bo_lang</th>\n",
       "      <th>conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p</td>\n",
       "      <td>50f27e734766438698d4d5c2d689a5f1</td>\n",
       "      <td>20201109</td>\n",
       "      <td>1604927870000</td>\n",
       "      <td>87be30dacdc74cd2936bcbe4ece6010d</td>\n",
       "      <td>Bullscricker</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p</td>\n",
       "      <td>3619348b91524430882f2b887838a3e4</td>\n",
       "      <td>20201018</td>\n",
       "      <td>1602989910000</td>\n",
       "      <td>8b67993183a14587a001010058d089d2</td>\n",
       "      <td>chucknellis</td>\n",
       "      <td>noqreport.com</td>\n",
       "      <td>post</td>\n",
       "      <td>Facebook’s Public Policy Manager for Global El...</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.8107684254646301]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p</td>\n",
       "      <td>1365051bfe6243599e6af1055b71c4a2</td>\n",
       "      <td>20201202</td>\n",
       "      <td>1606897009000</td>\n",
       "      <td>eeeb8dd25b7142b1bc69cbdbe1d8bb62</td>\n",
       "      <td>ThomasFox</td>\n",
       "      <td>rumble.com</td>\n",
       "      <td>post</td>\n",
       "      <td>USPS Driver Drops SHOCKING Claim About 200,000...</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.6809220314025879]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p</td>\n",
       "      <td>5f5e02a226454cbb9f56741a3aa6adce</td>\n",
       "      <td>20200824</td>\n",
       "      <td>1598283523000</td>\n",
       "      <td>e04afa3752f94366813196fb88022440</td>\n",
       "      <td>Kreermary</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p</td>\n",
       "      <td>3e689379be5b4e1fa333bbf7f5e1fad2</td>\n",
       "      <td>20201219</td>\n",
       "      <td>1608385852000</td>\n",
       "      <td>46563f08c3a34bebbf4960877d77230f</td>\n",
       "      <td>Starblazer692003</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>post</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254421</th>\n",
       "      <td>c</td>\n",
       "      <td>1ff3e15913c34456828bc70a7966bda1</td>\n",
       "      <td>20201009</td>\n",
       "      <td>1602255581000</td>\n",
       "      <td>e11890c2ffc348df9cb703651a3cc9c0</td>\n",
       "      <td>LibertyElaine</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254422</th>\n",
       "      <td>c</td>\n",
       "      <td>0a94034622254a0f96f943a2f7b98621</td>\n",
       "      <td>20200829</td>\n",
       "      <td>1598718069000</td>\n",
       "      <td>f8b37b14d52f493eab60c60486d5e6c6</td>\n",
       "      <td>Aj118</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254423</th>\n",
       "      <td>c</td>\n",
       "      <td>1dc1abde35234e32a872bcf76bd6ab45</td>\n",
       "      <td>20200820</td>\n",
       "      <td>1597891420000</td>\n",
       "      <td>efaac1ece6a893e0fab48f558e3a5c57</td>\n",
       "      <td>BethocAeilflaed</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>comment</td>\n",
       "      <td>This…\\n\\nFull of wine…\\n\\nOnly thing keeping m...</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.6988052725791931]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254424</th>\n",
       "      <td>c</td>\n",
       "      <td>4b80d4aa6f9d4775a8ba82f940272507</td>\n",
       "      <td>20200817</td>\n",
       "      <td>1597632198000</td>\n",
       "      <td>f8b37b14d52f493eab60c60486d5e6c6</td>\n",
       "      <td>Aj118</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>comment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254425</th>\n",
       "      <td>c</td>\n",
       "      <td>476d74eb1d2b4206b72cb02574c105a7</td>\n",
       "      <td>20200928</td>\n",
       "      <td>1601317643000</td>\n",
       "      <td>9ba302eb09584878ae72c873d550eb11</td>\n",
       "      <td>doutingthomas1</td>\n",
       "      <td>image-cdn.parler.com</td>\n",
       "      <td>comment</td>\n",
       "      <td>She’s going to hook them up to her Hoyer lift ...</td>\n",
       "      <td>English</td>\n",
       "      <td>[0.9862496852874756]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2254426 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         t                                id        cd              c  \\\n",
       "0        p  50f27e734766438698d4d5c2d689a5f1  20201109  1604927870000   \n",
       "1        p  3619348b91524430882f2b887838a3e4  20201018  1602989910000   \n",
       "2        p  1365051bfe6243599e6af1055b71c4a2  20201202  1606897009000   \n",
       "3        p  5f5e02a226454cbb9f56741a3aa6adce  20200824  1598283523000   \n",
       "4        p  3e689379be5b4e1fa333bbf7f5e1fad2  20201219  1608385852000   \n",
       "...     ..                               ...       ...            ...   \n",
       "2254421  c  1ff3e15913c34456828bc70a7966bda1  20201009  1602255581000   \n",
       "2254422  c  0a94034622254a0f96f943a2f7b98621  20200829  1598718069000   \n",
       "2254423  c  1dc1abde35234e32a872bcf76bd6ab45  20200820  1597891420000   \n",
       "2254424  c  4b80d4aa6f9d4775a8ba82f940272507  20200817  1597632198000   \n",
       "2254425  c  476d74eb1d2b4206b72cb02574c105a7  20200928  1601317643000   \n",
       "\n",
       "                                        u                un  \\\n",
       "0        87be30dacdc74cd2936bcbe4ece6010d      Bullscricker   \n",
       "1        8b67993183a14587a001010058d089d2       chucknellis   \n",
       "2        eeeb8dd25b7142b1bc69cbdbe1d8bb62         ThomasFox   \n",
       "3        e04afa3752f94366813196fb88022440         Kreermary   \n",
       "4        46563f08c3a34bebbf4960877d77230f  Starblazer692003   \n",
       "...                                   ...               ...   \n",
       "2254421  e11890c2ffc348df9cb703651a3cc9c0     LibertyElaine   \n",
       "2254422  f8b37b14d52f493eab60c60486d5e6c6             Aj118   \n",
       "2254423  efaac1ece6a893e0fab48f558e3a5c57   BethocAeilflaed   \n",
       "2254424  f8b37b14d52f493eab60c60486d5e6c6             Aj118   \n",
       "2254425  9ba302eb09584878ae72c873d550eb11    doutingthomas1   \n",
       "\n",
       "                          dmn     prov  \\\n",
       "0        image-cdn.parler.com     post   \n",
       "1               noqreport.com     post   \n",
       "2                  rumble.com     post   \n",
       "3        image-cdn.parler.com     post   \n",
       "4        image-cdn.parler.com     post   \n",
       "...                       ...      ...   \n",
       "2254421  image-cdn.parler.com  comment   \n",
       "2254422  image-cdn.parler.com  comment   \n",
       "2254423  image-cdn.parler.com  comment   \n",
       "2254424  image-cdn.parler.com  comment   \n",
       "2254425  image-cdn.parler.com  comment   \n",
       "\n",
       "                                                        bo  bo_lang  \\\n",
       "0                                                      NaN      NaN   \n",
       "1        Facebook’s Public Policy Manager for Global El...  English   \n",
       "2        USPS Driver Drops SHOCKING Claim About 200,000...  English   \n",
       "3                                                      NaN      NaN   \n",
       "4                                                      NaN      NaN   \n",
       "...                                                    ...      ...   \n",
       "2254421                                                NaN      NaN   \n",
       "2254422                                                NaN      NaN   \n",
       "2254423  This…\\n\\nFull of wine…\\n\\nOnly thing keeping m...  English   \n",
       "2254424                                                NaN      NaN   \n",
       "2254425  She’s going to hook them up to her Hoyer lift ...  English   \n",
       "\n",
       "                         conf  \n",
       "0                         NaN  \n",
       "1        [0.8107684254646301]  \n",
       "2        [0.6809220314025879]  \n",
       "3                         NaN  \n",
       "4                         NaN  \n",
       "...                       ...  \n",
       "2254421                   NaN  \n",
       "2254422                   NaN  \n",
       "2254423  [0.6988052725791931]  \n",
       "2254424                   NaN  \n",
       "2254425  [0.9862496852874756]  \n",
       "\n",
       "[2254426 rows x 11 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b58d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
